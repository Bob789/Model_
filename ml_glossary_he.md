# מילון מושגים בלמידת מכונה (Glossary)

המסמך מסכם ומרחיב באופן מקצועי את המונחים שנראו בתמונה, עם דגש על רגרסיה ליניארית ותהליך האימון. כל 항목 ממוספר לנוחות.

---

1. **למידה מונחית (Supervised Learning)**  
   למידה על נתונים מתוייגים, כלומר לכל דוגמת קלט \(\mathbf{x}\) יש תווית אמת \(y\). מודלים לומדים למפות \(\mathbf{x} \mapsto y\). משימות עיקריות: **רגרסיה** (חיזוי מספר רציף) ו**סיווג** (חיזוי קטגוריה).

2. **למידה לא־מונחית (Unsupervised Learning)**  
   למידה על נתונים ללא תוויות. המודל מחפש מבנים/דפוסים בנתונים. דוגמאות: **קלאסטרינג** (Clustering), **הפחתת ממד** (PCA), גילוי חריגים.

3. **תכונה / מאפיין (Feature, X)**  
   גודל מדיד המתאר את הדוגמה (למשל *גודל בית במ”ר*). בייצוג מטריציוני: \(\mathbf{X} \in \mathbb{R}^{n\times p}\) כאשר \(n\) דוגמאות ו־\(p\) תכונות. לעיתים מבצעים קנה־מידה (Normalization/Standardization).

4. **תווית / יעד (Label / Target, y)**  
   המשתנה התלוי שאותו רוצים לנבא (למשל *מחיר בית*). וקטור התוויות: \(\mathbf{y} \in \mathbb{R}^n\).

5. **משוואת הישר (Line Equation)**  
   בבעיה חד־ממדית: \(\hat{y} = b + wx\) כאשר \(w\) הוא **שיפוע** (slope) ו־\(b\) הוא **חיתוך עם הציר** (intercept).  
   בצורה וקטורית: \(\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{w} + b\). לעיתים מאחדים את \(b\) כעמודת 1 ב־\(\mathbf{X}\).

6. **הטיה (Bias Term, b)**  
   קבוע שמזיז את הפלט למעלה/למטה ללא תלות ב־\(\mathbf{x}\). מסייע ללכוד ממוצע בסיסי של המטרה.

7. **מקדם (Coefficient, Weight, w)**  
   פרמטר שמייצג את עוצמת ההשפעה של תכונה על התחזית. ביחידות: “שינוי צפוי ב־\(y\) לכל יחידת שינוי ב־\(x\) בהינתן שאר המשתנים קבועים”.

8. **ניבוי \(\hat{Y}\) מול אמת \(Y\)**  
   \(\hat{y}_i\) הוא הפלט של המודל עבור דוגמה \(i\); \(y_i\) הוא הערך האמיתי. נשווה ביניהם כדי להעריך את המודל.

9. **שאריות (Residuals)**  
   \(r_i = y_i - \hat{y}_i\). סכום ריבועי השאריות (RSS) ו**ממוצע** ריבועי השגיאה (**MSE**) הם מדדים מרכזיים לרגרסיה.

10. **פונקציית עלות (Cost / Loss Function)**  
    פונקציה שמודדים וממזערים במהלך האימון.  
    • ברגרסיה נפוצה: \(\text{MSE} = \frac{1}{n}\sum_i (y_i-\hat{y}_i)^2\).  
    • בסיווג: **שיעור טעות** (Error Rate) או **Log Loss / Cross-Entropy**.

11. **נוסחת הנורמל (Normal Equation)**  
    פתרון סגור ל־OLS בבסיס מטריציוני:  
    \[ \boldsymbol{\theta}^* = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y} \]  
    יעיל כאשר \(p\) קטן ו־\(\mathbf{X}^\top \mathbf{X}\) הפיכה. בבעיות גדולות/סינגולריות מעדיפים **Gradient Descent** או **Regularization** (למשל Ridge).

12. **מתאם (Correlation)**  
    מדד לקשר ליניארי בין שני משתנים: \(r = \frac{\mathrm{cov}(x,y)}{\sigma_x\sigma_y}\), ערכים בין −1 ל־1. **מתאם אינו סיבתיות**.

13. **התאמת־יתר (Overfitting)**  
    מצב שבו המודל “זוכר” רעשים ודקויות של סט האימון ולכן נכשל בהכללה לנתונים חדשים. סימנים: פער גדול בין ביצועי אימון ולוולידציה. פתרונות: פישוט המודל, איסוף עוד נתונים, רגולריזציה (L1/L2), עצירת־מוקדמת, קרוס־ולידציה, Data Augmentation.

14. **חסר־התאמה (Underfitting)**  
    מודל פשוט מדי שאינו לוכד את הדפוס הבסיסי. פתרונות: הוספת תכונות, הגדלת מורכבות המודל/דרגת הפולינום, הורדת רגולריזציה חזקה מדי.

15. **אימון (Train) / התאמה (Fit)**  
    תהליך מציאת פרמטרים \(\boldsymbol{\theta}\) שממזערים את פונקציית העלות. בפייתון, המתודה `fit()` מתאימה את המודל לנתוני האימון.

16. **הערכה (Evaluation)**  
    בדיקת ביצועי המודל על נתוני ולידציה/טסט שלא שימשו לאימון. ברגרסיה: **MAE**, **MSE**, **RMSE**, **R²**. מומלץ לבצע **Cross-Validation** ולבחון **תרשימי שאריות**.

17. **\(R^2\) — מקדם ההסבר (Coefficient of Determination)**  
    \(R^2 = 1 - \tfrac{\sum_i (y_i-\hat{y}_i)^2}{\sum_i (y_i-\bar{y})^2}\). מודד את חלק השונות ב־\(y\) שמוסבר ע״י המודל. יכול להיות שלילי כאשר המודל גרוע ממודל “ממוצע קבוע”.

18. **\(\text{Adj } R^2\) — \(R^2\) מתוקנן**  
    מתקן את \(R^2\) עבור מספר התכונות כדי להעניש הוספת משתנים לא מועילים. שימושי בהשוואה בין מודלים עם \(p\) שונה.

19. **גרדיאנט (Gradient)**  
    וקטור הנגזרות החלקיות של פונקציית העלות ביחס לפרמטרים. מצביע על כיוון העלייה התלולה ביותר של הפונקציה.

20. **ירידת מפל (Gradient Descent)**  
    אלגוריתם אופטימיזציה שמעדכן פרמטרים בכיוון הנגדי לגרדיאנט:  
    \[ \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) \]  
    וריאציות: Batch, Stochastic (SGD), Mini-Batch; תאוצות כמו Momentum, Adam.

21. **קצב למידה (Learning Rate, \(\eta\))**  
    גודל הצעד בכל עדכון. קטן מדי → התכנסות איטית; גדול מדי → תנודות/התבדרות. נהוג להשתמש ב־Schedulers או חיפוש היפר־פרמטרים.

22. **רגרסיה ליניארית — משתנה יחיד (Simple Linear Regression)**  
    התאמת ישר ל־\((x,y)\) ע״י מזעור MSE. פתרון OLS נותן אומדן חסר־הטיה תחת הנחות קלאסיות. שימושית להסבר והשוואה.

23. **רגרסיה ליניארית מרובת משתנים (Multiple Linear Regression)**  
    כללית יותר: \(\hat{y} = b + \sum_{j=1}^p w_j x_j\). דורשת תשומת לב לקו־ליניאריות, סקיילינג, בחירת משתנים ורגולריזציה.

24. **רגרסיה פולינומית (Polynomial Regression)**  
    הרחבת המודל ע״י יצירת תכונות פולינומיות (למשל \(1, x, x^2, x^3\)). מאפשרת להתאים קשרים לא־ליניאריים. חשוב לנרמל תכונות ולשלב רגולריזציה כדי למנוע התאמת־יתר.

25. **חלוקת נתונים (Train / Validation / Test Split)**  
    נהוג לפצל את הנתונים כדי להעריך הכללה: אימון, ולידציה לכיוונון היפר־פרמטרים, וטסט להערכת ביצועים סופית.

26. **דוגמה קצרה (בית — מחיר)**  
    *Feature:* גודל הבית במ״ר. *Target:* מחיר. נבנה מודל רגרסיה ליניארית, נחשב \(\hat{y}\), נבדוק שאריות, נמדוד MAE/RMSE ו־\(R^2\), ונעקוב אחרי התאמת־יתר/חסר בעזרת ולידציה.

---

## נוסחאות שימושיות מקוצרות
- **MSE:** \(\tfrac{1}{n}\sum_i (y_i-\hat{y}_i)^2\) ; **RMSE:** \(\sqrt{\text{MSE}}\) ; **MAE:** \(\tfrac{1}{n}\sum_i |y_i-\hat{y}_i|\)  
- **נוסחת הנורמל:** \(\boldsymbol{\theta}^* = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)  
- **עדכון GD:** \(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \eta \nabla J\)  
- **מתאם פירסון:** \(r = \frac{\mathrm{cov}(x,y)}{\sigma_x\sigma_y}\)

> טיפים: וודאו סקיילינג עקבי לתכונות, בדקו גרפים של שאריות, והעדיפו קרוס־ולידציה להשוואת מודלים.

